"""æ¨¡å‹å¾®è°ƒæ¨¡å— - å¸¦è¯¦ç»†æ—¥å¿—"""

import os
import sys
import json
import warnings
import traceback
import gc

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import torch
from typing import Optional
from pathlib import Path
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType
)

from .config import get_config


class ModelTrainer:
    """æ¨¡å‹å¾®è°ƒå™¨"""
    
    def __init__(self):
        self.config = get_config()
        self.model = None
        self.tokenizer = None
        self.trainer = None
    
    def check_gpu(self):
        """æ£€æŸ¥GPUçŠ¶æ€"""
        print("\n" + "=" * 50)
        print("GPU ä¿¡æ¯")
        print("=" * 50)
        
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"âœ“ GPU: {gpu_name}")
            print(f"âœ“ æ˜¾å­˜: {gpu_memory:.1f} GB")
            print(f"âœ“ CUDAç‰ˆæœ¬: {torch.version.cuda}")
            torch.cuda.empty_cache()
            gc.collect()
            return True
        else:
            print("âš  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
            return False
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print("\n" + "=" * 50)
        print("åŠ è½½æ¨¡å‹")
        print("=" * 50)
        
        model_name = self.config.model.base_model
        cache_dir = self.config.paths.base_model_cache
        
        print(f"åŸºç¡€æ¨¡å‹: {model_name}")
        print(f"ç¼“å­˜ç›®å½•: {cache_dir}")
        
        try:
            # åŠ è½½åˆ†è¯å™¨
            print("\n[1/4] åŠ è½½åˆ†è¯å™¨...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                padding_side="right",
                cache_dir=cache_dir
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            print("   âœ“ åˆ†è¯å™¨åŠ è½½å®Œæˆ")
            
            # åŠ è½½æ¨¡å‹
            print("\n[2/4] åŠ è½½æ¨¡å‹...")
            print(f"   - CUDAå¯ç”¨: {torch.cuda.is_available()}")
            
            if torch.cuda.is_available():
                device_map = "auto"
                dtype = torch.bfloat16
                print(f"   - ä½¿ç”¨GPU + bfloat16")
            else:
                device_map = None
                dtype = torch.float32
                print(f"   - ä½¿ç”¨CPU + float32")
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=dtype,
                device_map=device_map,
                trust_remote_code=True,
                cache_dir=cache_dir
            )
            print("   âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
            
            # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
            print("\n[3/4] é…ç½®è®­ç»ƒå‚æ•°...")
            self.model.gradient_checkpointing_enable()
            self.model.enable_input_require_grads()
            print("   âœ“ æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
            
            # é…ç½®LoRA
            print("\n[4/4] é…ç½®LoRA...")
            peft_config = LoraConfig(
                r=self.config.lora.r,
                lora_alpha=self.config.lora.alpha,
                lora_dropout=self.config.lora.dropout,
                target_modules=self.config.lora.target_modules,
                bias="none",
                task_type=TaskType.CAUSAL_LM
            )
            
            self.model = get_peft_model(self.model, peft_config)
            print("   âœ“ LoRAé…ç½®å®Œæˆ")
            
            self._print_trainable_parameters()
            
            print("\n" + "=" * 50)
            print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ!")
            print("=" * 50)
            
        except Exception as e:
            print(f"\n" + "=" * 50)
            print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥!")
            print("=" * 50)
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
            print("\nè¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            raise
    
    def _print_trainable_parameters(self):
        """æ‰“å°å¯è®­ç»ƒå‚æ•°"""
        trainable_params = 0
        all_params = 0
        
        for _, param in self.model.named_parameters():
            all_params += param.numel()
            if param.requires_grad:
                trainable_params += param.numel()
        
        print(f"\nå¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {all_params:,} "
              f"({100 * trainable_params / all_params:.2f}%)")
    
    def load_dataset(self, data_path: str = None) -> Dataset:
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        data_path = data_path or os.path.join(
            self.config.paths.processed_data, "train_data.json"
        )
        
        if not os.path.exists(data_path):
            raise FileNotFoundError(
                f"è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {data_path}\n"
                f"è¯·å…ˆè¿è¡Œ: python main.py process_data"
            )
        
        print(f"\nåŠ è½½æ•°æ®: {data_path}")
        
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"åŸå§‹æ ·æœ¬æ•°: {len(data)}")
        
        processed_data = []
        for item in data:
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘ç”³è¯·ä¹¦å†™ä½œåŠ©æ‰‹ã€‚"},
                {"role": "user", "content": f"{item['instruction']}\n\n{item.get('input', '')}".strip()},
                {"role": "assistant", "content": item['output']}
            ]
            
            try:
                text = self.tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=False
                )
            except:
                text = f"### æŒ‡ä»¤:\n{item['instruction']}\n\n"
                if item.get('input'):
                    text += f"### è¾“å…¥:\n{item['input']}\n\n"
                text += f"### å›ç­”:\n{item['output']}"
            
            processed_data.append({"text": text})
        
        dataset = Dataset.from_list(processed_data)
        print(f"å¤„ç†åæ ·æœ¬æ•°: {len(dataset)}")
        
        return dataset
    
    def train(self, data_path: str = None):
        """æ‰§è¡Œè®­ç»ƒ"""
        print("\n" + "=" * 50)
        print("ğŸš€ å¼€å§‹è®­ç»ƒæµç¨‹")
        print("=" * 50)
        
        # æ£€æŸ¥GPU
        has_gpu = self.check_gpu()
        
        # åŠ è½½æ¨¡å‹
        self.setup_model()
        
        # åŠ è½½æ•°æ®
        dataset = self.load_dataset(data_path)
        
        # åˆ†è¯
        print("\nå¯¹æ•°æ®è¿›è¡Œåˆ†è¯...")
        
        def tokenize_function(examples):
            result = self.tokenizer(
                examples["text"],
                truncation=True,
                max_length=self.config.model.max_length,
                padding="max_length",
                return_tensors=None
            )
            result["labels"] = result["input_ids"].copy()
            return result
        
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names,
            desc="åˆ†è¯å¤„ç†"
        )
        print(f"âœ“ åˆ†è¯å®Œæˆï¼Œå…± {len(tokenized_dataset)} æ¡æ•°æ®")
        
        # è®­ç»ƒå‚æ•°
        output_dir = self.config.paths.finetuned_model
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=self.config.training.num_epochs,
            per_device_train_batch_size=self.config.training.batch_size,
            gradient_accumulation_steps=self.config.training.gradient_accumulation_steps,
            learning_rate=self.config.training.learning_rate,
            warmup_ratio=self.config.training.warmup_ratio,
            lr_scheduler_type="cosine",
            logging_steps=self.config.training.logging_steps,
            save_steps=self.config.training.save_steps,
            save_total_limit=3,
            fp16=False,
            bf16=has_gpu,
            gradient_checkpointing=True,
            max_grad_norm=self.config.training.max_grad_norm,
            optim="adamw_torch",
            report_to="none",
            remove_unused_columns=False,
            dataloader_pin_memory=False,
        )
        
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer, mlm=False
        )
        
        # åˆ›å»ºTrainer
        print("\nåˆ›å»ºè®­ç»ƒå™¨...")
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
        )
        
        print("\n" + "=" * 50)
        print("å¼€å§‹è®­ç»ƒ")
        print("=" * 50)
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        print(f"è®­ç»ƒè½®æ•°: {self.config.training.num_epochs}")
        print(f"æ‰¹æ¬¡å¤§å°: {self.config.training.batch_size}")
        print(f"æ¢¯åº¦ç´¯ç§¯: {self.config.training.gradient_accumulation_steps}")
        print(f"å­¦ä¹ ç‡: {self.config.training.learning_rate}")
        
        # å¼€å§‹è®­ç»ƒ
        self.trainer.train()
        
        # ä¿å­˜
        print("\nä¿å­˜æ¨¡å‹...")
        self.trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        print(f"\nâœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
    
    def merge_and_save(self, output_dir: str = None):
        """åˆå¹¶LoRAæƒé‡"""
        output_dir = output_dir or self.config.paths.merged_model
        
        print("\n" + "=" * 50)
        print("åˆå¹¶LoRAæƒé‡")
        print("=" * 50)
        
        torch.cuda.empty_cache()
        gc.collect()
        
        merged_model = self.model.merge_and_unload()
        
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        merged_model.save_pretrained(output_dir, safe_serialization=True)
        self.tokenizer.save_pretrained(output_dir)
        
        print(f"âœ“ åˆå¹¶åçš„æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
    
    def run(self, data_path: str = None, merge: bool = True):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("\n" + "=" * 50)
        print("ğŸš€ å›½è‡ªç„¶å†™ä½œåŠ©æ‰‹ - æ¨¡å‹å¾®è°ƒ")
        print("=" * 50)
        
        try:
            self.train(data_path)
            
            if merge:
                self.merge_and_save()
            
            print("\n" + "=" * 50)
            print("âœ“ è®­ç»ƒå®Œæˆ!")
            print("=" * 50)
            print("\nä¸‹ä¸€æ­¥æ“ä½œ:")
            print("  1. å¯åŠ¨Webåº”ç”¨: python main.py run")
            print("  2. æˆ–éƒ¨ç½²åˆ°Ollama: python main.py deploy")
            
        except KeyboardInterrupt:
            print("\n\nâš  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            
        except Exception as e:
            print(f"\nâŒ è®­ç»ƒå¤±è´¥: {str(e)}")
            traceback.print_exc()
            raise